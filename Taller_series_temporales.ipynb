{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chikorichi/Hola-mundo-/blob/main/Taller_series_temporales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# Taller de análisis de series temporales\n",
        "\n",
        "<hr>\n",
        "\n",
        "## Congreso Internacional de Informática y Sistemas XXVI\n",
        "### Universidad Jorge Basadre Grohmann, Tacna, Perú\n",
        "### Noviembre de 2025\n",
        "\n",
        "<hr>\n",
        "\n",
        "#### Prof. Ignacio Ramírez Paulino nacho@fing.edu.uy\n",
        "#### Instituto de Ingeniería Eléctrica\n",
        "#### Facultad de Ingeniería\n",
        "#### Universidad de la República Oriental del Uruguay (UdelaR)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nAsf98Cb1er-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrTEme4O9Pd-"
      },
      "source": [
        "# Predicción de demanda eléctrica en Uruguay\n",
        "\n",
        "## Sobre este taller\n",
        "\n",
        "Este es un taller opcionalmente interactivo. Esto significa que no nos detendremos a resolver ejercicios. Lo que se hará es presentar una serie de pasos para concretar un objetivo, y se propondrá variaciones a los estudiantes para que experimenten con distintos parámetros y decisiones de diseño a lo largo de ellos. Dichas propuestas están resaltadas en el texto en los lugares correspondientes.\n",
        "\n",
        "## Contenido\n",
        "\n",
        "*   [Introducción al problema](#intro)\n",
        "*   [Lectura de los datos](#lectura)\n",
        "*   [Visualización previa exploratoria](#visualizacion)\n",
        "*.  [Descomposición TSR](#tsr)\n",
        "*   [Preprocesamiento](#preprocesamiento)\n",
        "*   [Modelado](#modelado)\n",
        "*   [Evaluación](#evaluacion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3SXxtveBVa-"
      },
      "source": [
        "<a id=\"intro\"></a>\n",
        "# Introducción\n",
        "\n",
        "## Predicción de demanda eléctrica\n",
        "\n",
        "Los sistemas eléctricos están en constante funcionamiento. Sistemas, plantas, conexiones, se apagan y prenden, abren y cierran diariamente para ajustarse a la demanda de electricidad del país en su totalidad.\n",
        "\n",
        "Por diversos motivos, lo anterior no puede hacerse de manera instantánea, sino que requiere cierta previsión. Es por eso que prever la demanda del sistema con cierta antelación, usualmente de un día para el otro, es crucial para un buen funcionamiento del sistema eléctrico. Este es un problema que se da en todo el mundo, no sólo en Uruguay.\n",
        "\n",
        "La demanda eléctrica se mide en MWh (millones de Watts-hora). Cabe mencionar que un watt-hora es una medida de _energía_, tal como el Joule (no de potencia instantánea, como lo es el Watt). Para fijar ideas, un Watt-hora equivale a la energía consumida durante una hora por un aparato cuya potencia instantánea es 1 watt. Por ejemplo, una bombita de luz de 25 watts, prendida durante 1 hora, consume 25 watts-hora. Esa misma bombita, prendida durante 4 horas, consume 100 watts-hora, etc.\n",
        "\n",
        "Esa es la unidad en que vienen nuestras facturas de UTE.\n",
        "\n",
        "## Antecedentes\n",
        "\n",
        "El siguiente trabajo fue realizado junto con colegas del Instituto de Estadística de UdelaR (IESTA) en un proyecto del Fondo Sectorial de Energía de 2013 (FSE 2013). Los resultados de ese trabajo están publicados en [[CCMR2018]](#CCMR2018).\n",
        "\n",
        "## Planteo del problema\n",
        "\n",
        "El tipo de problema al que nos enfrentamos es de _predicción_. Si bien es un problema clásico de estadística, procesamiento de señales, y análisis de series temporales, en la jerga de Aprendizaje Automático se ha dado a llamar a este tipo de problemas _autosupervisados_ (self supervised). La razón es la siguiente: son problemas en los que el entrenamiento tiene un objetivo bien definido, que es acertar (predecir) con la mayor exactitud a las muestras futuras de la serie de datos. Durante el entrenamiento, el sistema tiene como entrada las muestras pasadas, y su salida se compara con las muestras futuras de una serie de entrenamiento _ya conocida de antemano_. Es decir, los propios datos hacen las veces de entrada al sistema, y de salida de referencia, y es por eso que es \"autosupervisado\".\n",
        "\n",
        "## Escenario\n",
        "\n",
        "En el caso concreto de predicción de demanda que nos atañe, debemos predecir la demanda del día siguiente, que no es un sólo dato sino 144 (son 10 datos por hora), a partir de los datos disponibles hasta cierta hora del día actual.\n",
        "\n",
        "Los datos de que disponemos son variados: para empezar, tenemos la serie de demandas efectivamente registradas hasta el día actual. Además, tenemos medidas de presión, temperatura, humedad, y radiación solar del día actual, y predicciones para el día siguiente. Finalmente, tenemos información de calendario, incluyendo si es un feriado o no. Como veremos más adelante, toda esta información puede ser relevante para predecir la demanda del día siguiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juBYwa66QwR0"
      },
      "source": [
        "!wget -c  http://iie.fing.edu.uy/~nacho/data/energia/demanda_2007_2012_pre1.csv.gz\n",
        "!wget -c  http://iie.fing.edu.uy/~nacho/data/energia/demanda_2007_2012_pre2.csv.gz\n",
        "!wget -c  http://iie.fing.edu.uy/~nacho/data/energia/demanda_2007_2012_pre3.csv.gz\n",
        "!wget -c  http://iie.fing.edu.uy/~nacho/data/energia/demanda_2007_2012_pre4.csv.gz\n",
        "#\n",
        "# preámbulo\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as rng\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.dpi']  = 100\n",
        "mpl.rcParams['savefig.dpi'] = 300\n",
        "\n",
        "mpl.rcParams['font.size']        = 10\n",
        "mpl.rcParams['legend.fontsize']  = 'medium'\n",
        "mpl.rcParams['figure.titlesize'] = 'medium'\n",
        "\n",
        "#\n",
        "# DATOS?\n",
        "#\n",
        "#https://www.kaggle.com/adityakadiwal/water-potability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHb-7L1KBkPN"
      },
      "source": [
        "<a id=\"lectura\"></a>\n",
        "## Lectura\n",
        "\n",
        "Los datos están en formato CSV (Comma Separated Value), una forma muy sencilla pero efectiva de almacenar tablas de datos.\n",
        "\n",
        "Para ahorrar espacio, los archivos están comprimidos con el formato `gzip`; esto se denota por su extensión `.gz`.  El _gzip_  es un pariente del popular formato _zip_ que tiene soporte nativo en todos los sistemas operativos más comunes, lo que lo hace una buena elección desde el punto de vista de la _portabilidad_  de los datos, y por ende de la reproducibilidad de los experimentos.\n",
        "\n",
        "Los archivos CSV se organizan en _filas_. Cada fila se lee como una tupla (lista) de $n$ elementos, donde $n$ es la cantidad de columnas de la tabla.\n",
        "\n",
        "Python ya incluye funciones básicas para leer este tipo de datos a través del módulo `csv`. Este módulo es bastante flexible, pero requiere un poco de programación.\n",
        "\n",
        "Como alternativa, el paquete `pandas` ya incluye funciones para leer archivos CSV enteros dentro de estructuras `DataFrame` de Pandas. Además, Pandas facilita mucho la interpretación del contenido de los archivos. Vayamos entonces por ese camino.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drdluwZ-wFuN"
      },
      "source": [
        "import csv\n",
        "import gzip\n",
        "import pandas as pd\n",
        "\n",
        "# mode='rt' indica 2 cosas:\n",
        "# -la 'r' indica 'read' (abrir para lectura)\n",
        "# -la 't' indica 'abrir como texto'\n",
        "#\n",
        "with gzip.open('demanda_2007_2012_pre1.csv.gz',mode='rt') as gzfile:\n",
        "  #\n",
        "  # las columnas del archivo CSV están separadas por caracteres tabuladores('tab')\n",
        "  # que se escriben con el código '\\t'\n",
        "  #\n",
        "  # además, sabemos que los datos inválidos en el archivo (por ej., temperaturas que no pudieron\n",
        "  # ser medidas), tienen valor -100.00. Eso se lo indicamos a Pandas con el parámetro 'na_values'\n",
        "  #\n",
        "  dem_data = pd.read_csv(gzfile,delimiter='\\t',na_values=('-100.00'))\n",
        "#\n",
        "# inspeccionemos las primeras filas\n",
        "#\n",
        "dem_data.head()\n",
        "#\n",
        "# convirtamos el archivo a una matriz numérica; esto nos facilitará imprimir\n",
        "# (esto nos puede venir bien más tarde)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OulYp3PhBkR-"
      },
      "source": [
        "<a id=\"visualizacion\"></a>\n",
        "## Visualización\n",
        "\n",
        "Los datos que tenemos son series temporales de demanda de energía junto con otros datos de utilidad para realizar la predicción. Concretamente, cada fila del archivo contiene los siguientes campos:\n",
        "\n",
        "1.  año\n",
        "1.  mes\n",
        "1.  dia\n",
        "1.  año ISO (International Standards Organization)\n",
        "1.  semana ISO\n",
        "1.  dia    ISO\n",
        "1.  hora\n",
        "1.  minuto\n",
        "1.  feriado (1 si es feriado, 0 si no)\n",
        "1.  temperatura\n",
        "1.  índice de radiación solar\n",
        "1.  demanda promedio en MWh durante los últimos 10 minutos\n",
        "1.  temperatura válida? (1: sí, 0: no)\n",
        "1.  radiación válida?\n",
        "1.  demanda válida?\n",
        "\n",
        "Los primeras 8 datos tienen que ver con el tiempo (no hay lugar aquí para explicar por qué existen años y semanas ISO aparte de los _comunes_). Los últimos tres datos son indicadores auxiliares que nos dicen si el dato correspondiente en la fila es una medida válida; esto es fundamental para la etapa de preprocesamiento, que veremos más adelante.\n",
        "\n",
        "La presuposición más fuerte y utilizada con estos datos es que son _estacionales_, es decir, que varían de manera más o menos predecible con las estaciones o temporadas. En particular, tenemos tres estacionariedades principales que afectan a la demanda eléctrica:\n",
        "\n",
        "*   El mes del año\n",
        "*   El día de la semana\n",
        "*   La hora del día\n",
        "\n",
        "Para confirmar esto, veamos respectivamente cómo evoluciona la demanda durante un año, una semana, y un día. Los datos que tenemos son _diezminutales_, es decir, tenemos un dato cada diez minutos. En consecuencia:\n",
        "*   Un día abarca $24{\\times}6=144$ datos\n",
        "*   Una semana abarca 7 días o sea $7{\\times}144=1080$ datos\n",
        "*   Un año abarca _aproximadamente_ $365$ días (como todos sabemos, esto es una aproximación, que se corrige en los años bisiestos agregando un día). Ahora, para simplificar, sigamos adelante. Esto nos da $365{\\times}144=52560$.\n",
        "\n",
        "## Demanda a lo largo de un día\n",
        "\n",
        "Abajo vemos la demanda del día no. 50 de 2007, el 19 de febrero, que cayó en un lunes. La curva refleja el patrón típico de un día de semana: la actividad cae durante la madrugada hasta un mínimo a las 4:00. Luego crece hacia al mediodía, decae levemente sobre el final de la tarde, y luego sube a su máximo a eso de las 21."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtcu5AOM1MPo"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "dem  = dem_data['AVGDEM'][50*144:51*144]\n",
        "plt.plot(np.linspace(0,23.50,len(dem)),dem)\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('50-avo día de 2007')\n",
        "plt.savefig('un_dia.png')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy6rys7Y6dLR"
      },
      "source": [
        "### Demanda de una semana\n",
        "\n",
        "Si miramos los datos de la primera semana, podemos observar un patrón cíclico claro, correspondiente a los siete días: hay 7 picos, y entre ellos se distingue un patrón que se repite casi de manera idéntica, salvo en las puntas, que corresponden a domingo y sábado respectivamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEdIqf3Q7L5Y"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "dem_data['AVGDEM'][:1008].plot()\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('primera semana de 2007')\n",
        "plt.savefig('una_semana.png')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYXr2TiW7Ndg"
      },
      "source": [
        "### Demanda a lo largo de un mes\n",
        "\n",
        "Si observamos los datos en un mes, vemos que hay también un patrón repetitivo asociado a las semanas: cada semana se comporta de manera muy similar a las otras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eQKwcYU7Nmn"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "dem_data['AVGDEM'][:(144*31)].plot()\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('primer mes de 2007')\n",
        "plt.savefig('un_mes.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozL2Qm1r7_hQ"
      },
      "source": [
        "### Demanda durante varios años\n",
        "\n",
        "El patrón de demanda también tiene un patrón repetitivo año a año. Para ver esto, mostramos los datos de los 6 años que tenemos.\n",
        "\n",
        "Si bien es más difícil ver el patrón aquí debido a las variaciones anteriores, aún es posible distinguirlo a simple vista. Por lo pronto, hay 6 picos y un patrón que se repite en torno a cada uno de ellos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JMCdZWV7_p4"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "x = dem_data['AVGDEM']\n",
        "plt.plot(x,lw=0.5)\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('seis años (2007-2012)')\n",
        "plt.savefig('un_anio.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Componentes estacionales\n",
        "\n",
        "Veamos cómo se ven las distintas periodicidades en la transformada de Fourier de la serie.\n",
        "\n",
        "La transformada de fourier se muestra en _frecuencia discreta_.\n",
        "La frecuencia es en ciclos por muestra.\n",
        "\n",
        "El código siguiente resalta las componentes más prominentes, que como puede esperarse corresponden a las periodicidades diaria, semanal y anual.\n",
        "\n",
        "Las frecuencias discretas se escriben como $k/n$, donde $n$ es la cantidad de muestras. En el ejemplo de abajo se muestran $4$ años por lo que esperamos que el año aparezca en la cuarta componente del vector. Asimismo aparecerán las semanas en la posición $7*52$ y los días en $7*365$"
      ],
      "metadata": {
        "id": "UCp_oRvyg0HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import fft\n",
        "#\n",
        "# temporalmente cargamos unos datos limpiados (sin NaNs)\n",
        "#\n",
        "with gzip.open('demanda_2007_2012_pre4.csv.gz',mode='rt') as gzfile:\n",
        "  demanda_sin_nan = pd.read_csv(gzfile,delimiter='\\t',na_values=('-100.00','-200.00'))\n",
        "x = np.array(demanda_sin_nan['AVGDEM'])\n",
        "# los datos son 10 minutales, o sea 6 por hora\n",
        "n_hora = 6\n",
        "# muestras en un dia\n",
        "n_dia = n_hora*24\n",
        "# muestras en una semana\n",
        "n_sem = n_dia*7\n",
        "# muestras en un año\n",
        "n_anio = n_dia*365\n",
        "# mostramos 6 años\n",
        "n = n_anio*5\n",
        "x = x[:n]\n",
        "\n",
        "n = len(x)\n",
        "#plt.figure()\n",
        "#plt.plot(x)\n",
        "#plt.show()\n",
        "#\n",
        "# Transformada de Fourier\n",
        "#\n",
        "x_fourier = np.abs(fft.fft(x))/n\n",
        "# la transformada discreta de Fourier no tiene 'tiempo'\n",
        "# para darle un sentido físico a las frecuencias, usamos esta función\n",
        "# en donde especificamos la unidad de tiempo que queremos ver.\n",
        "# usemos 1 día. De esta manera, la frecuencia diaria aparecerá en e '1'\n",
        "T_s = 1/n_dia\n",
        "f_fourier = fft.fftfreq(n,T_s)\n",
        "\n",
        "#\n",
        "# mostramos la transformada y lugares correspondientes a\n",
        "# las componentes frecuenciales principales\n",
        "#\n",
        "#\n",
        "f_anio = 5\n",
        "f_sem = f_anio*52\n",
        "f_dia = f_sem*7\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(f_fourier[1:2*f_dia+100],x_fourier[1:2*f_dia+100])\n",
        "plt.scatter(f_fourier[f_dia],(0),color=(1,0,0))\n",
        "plt.scatter(f_fourier[2*f_dia],(0),color=(1,0,0))\n",
        "for i in range(7):\n",
        "  plt.scatter(f_fourier[i*f_sem],(0),color=(0,1,0))\n",
        "plt.xlabel('frecuencia (ciclos por día)')\n",
        "plt.ylabel('valor absoluto de coeficiente')\n",
        "plt.savefig('fourier_dia.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(f_fourier[1:f_anio*4],x_fourier[1:f_anio*4])\n",
        "plt.scatter(f_fourier[1*f_anio],(0),color=(0,0,1))\n",
        "plt.scatter(f_fourier[2*f_anio],(0),color=(0,0,1))\n",
        "plt.xlabel('frecuencia (ciclos por día)')\n",
        "plt.ylabel('valor absoluto de coeficiente')\n",
        "plt.savefig('fourier_anio.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_0jI7jvgg8Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB5IwuwMtTXv"
      },
      "source": [
        "### Tendencia\n",
        "\n",
        "Volviendo a la gráfica de la demanda completa, vemos que no sólo hay un patrón periódico que se repite cada año: a gran escala se observa también un aumento paulatino de la demanada con el correr de los años. Esta tendencia también debemos tenerla en cuenta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paréntesis: descomposición R,S,T\n",
        "\n",
        "Anteriormente vimos que la serie en estudio tiene una tendencia de crecimento lento pero sostenido a lo largo de los años. También vimos que tiene una periodicidad clara. Lo que veremos ahora es un procedimiento estándar para identificar y descomponer la serie en dichas componentes. Esta técnica no será la que en última instancia utilicemos para realizar la inferencia en nuestro caso, pero es muy útil y puede utilizarse perfectamente.\n",
        "En resumen, la idea es descomponer la serie como la suma de tres componentes:\n",
        "\n",
        "* Estacional (S)\n",
        "* Tendencia (T)\n",
        "* Proceso AR (R)\n",
        "\n",
        "Claramente hay infinitas formas de descomponer una serie en tres partes aditivas. La pregunta es cómo realizar dicha descomposición de manera que las partes obtenidas sean útiles y representen lo que queremos. Lo que haremos aquí es muy sencillo pero eficaz:\n",
        "\n",
        "* Primero quitaremos la componente continua y una tendencia lineal ajustando los datos a una recta.\n",
        "* Luego haremos la transformada de Fourier, y nos quedaremos con las componentes frecuenciales más prominentes según algún criterio\n",
        "* El resto lo modelaremos como un proceso autoregresivo (AR)\n",
        "\n",
        "Los dos últimos pasos pueden hacerse de manera iterativa, eligiendo distintas cantidades de coeficientes de Fourier y orden del filtro autoregresivo. Aquí haremos todo con valores fijos.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rHq-fiGcDkUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Captura de la tendencia T\n",
        "\n",
        "Queremos encontrar un vector  $a \\in \\mathbb{R}^2$ tal que que $$x[k] \\approx a_0 + a_1 k.$$\n",
        "Lo anterior lo hacemos resolviendo el sistema sobredeterminado\n",
        "$$\\left[\\begin{array}{cc}1&0\\\\1&1\\\\1&2\\\\\\vdots&\\vdots\\\\1&k\\end{array}\\right] \\left[\\begin{array}{c}a_0\\\\a_1\\end{array}\\right]\n",
        " =\n",
        " \\left[\\begin{array}{c}x[0]\\\\x[1]\\\\x[2]\\\\\\vdots\\\\\\x[k]\\end{array}\\right]$$\n",
        "\n",
        "Si llamamos $H$ a la primera matriz, esto se transforma en $Ha=x$.\n"
      ],
      "metadata": {
        "id": "-9dNvBHAFqos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy.linalg as la\n",
        "\n",
        "x = np.array(demanda_sin_nan['AVGDEM'])\n",
        "#\n",
        "# construimos la matriz H\n",
        "#\n",
        "n = len(x)\n",
        "H = np.stack((np.ones(n), np.arange(n))).T\n",
        "#\n",
        "# resolvemos el problema de mínimos cuadrados\n",
        "#\n",
        "a = la.lstsq(H,x)[0]\n",
        "#\n",
        "# obtenemos la componente de tendencia como T=Ha\n",
        "#\n",
        "T = H @ a\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(x,label='datos',lw=0.5)\n",
        "plt.plot(T,label='tendencia',lw=2)\n",
        "plt.title('ajuste de tendencia')\n",
        "plt.legend()\n",
        "plt.savefig('T.png')\n",
        "plt.show()\n",
        "#\n",
        "# extremos el residuo, que no contiene tendencia\n",
        "#\n",
        "x_1 = x - T"
      ],
      "metadata": {
        "id": "nR4ibuFGFisv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Componente estacional S\n",
        "\n",
        "Ahora calculamos la transformada de Fourier y nos quedamos con los K coeficientes más grandes es importante tener en cuenta que, si bien hay 3 componentes periódicas, cada una tiene varias sub-componentes (las llamadas frecuencias parciales)\n",
        "es decir, si hay periodicidad a frecuencia diaria, también hay cada dos días, cada tres, etc. por lo que la cantidad de componentes necesarios es mucho mayor que 3. En el ejemplo nos quedamos con un 1% de las componente.\n",
        "\n",
        "**Ejercicio: probar otros valores de K (menores!)**\n"
      ],
      "metadata": {
        "id": "gCuQG3aOLX9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# calculamos la transformada de Fourier\n",
        "#\n",
        "n = len(x)\n",
        "x_1_f = fft.fft(x_1)\n",
        "X_1_f = np.abs(x_1_f)\n",
        "#\n",
        "# ponemps en cero todos los valores menos los K mayores\n",
        "#\n",
        "X_1_f_sorted = np.argsort(X_1_f)\n",
        "K = n//100 # PROPUESTA: CAMBIAR!\n",
        "x_1_f[X_1_f_sorted[:-K]] = 0\n",
        "#\n",
        "# hacemos la transformada inversa para obtener la aproximación periódica de x_1\n",
        "#\n",
        "S = np.real(fft.ifft(x_1_f))\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(x_1[:4000],lw=1,label='x-T')\n",
        "plt.plot(S[:4000],lw=1,label='S')\n",
        "plt.title('Serie y su componente estacional')\n",
        "plt.legend()\n",
        "plt.xlabel('muestra')\n",
        "plt.savefig('S.png')\n",
        "plt.show()\n",
        "#\n",
        "# x_2 es el residuo, que no contiene ni componente tendencial ni periódica\n",
        "#\n",
        "x_2 = x_1 - S\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(x_2[:4000],lw=1,label='x-T')\n",
        "plt.title('Residuo x - T - S')\n",
        "plt.legend()\n",
        "plt.xlabel('muestra')\n",
        "plt.savefig('x_T_S.png')\n",
        "plt.show()\n",
        "print('Error cuadrático medio:',np.std(x_2))\n"
      ],
      "metadata": {
        "id": "Y6SQhgMKIDPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Componente aleatoria R\n",
        "\n",
        "Resta modelar el residuo resultante como un proceso autoregresivo. El principal parámetro aquí es el orden del proceso, el cual elegimos como 24 por corresponder a un día, pero en realidad es arbitrario.\n",
        "\n",
        "**EJERCICIO: probar con otros valores de P y observar el resultado**"
      ],
      "metadata": {
        "id": "M-FA91BDLU0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P = 24\n",
        "b = x_2[P:]\n",
        "A = np.zeros((n-P,P))\n",
        "for p in range(P):\n",
        "  A[:,p] = x_2[p:n-P+p]\n",
        "\n",
        "a = la.lstsq(A,b)[0]\n",
        "R = A @ a\n",
        "R = np.pad(R,(P,0))\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(x_2[:1000],lw=1,label='x-T-S')\n",
        "plt.plot(R[:1000],lw=1,label='R')\n",
        "plt.title('Serie y su componente estacional')\n",
        "plt.legend()\n",
        "plt.xlabel('muestra')\n",
        "plt.savefig('R.png')\n",
        "plt.show()\n",
        "\n",
        "# residuo\n",
        "x_3 = x_2 - R\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(x_3[:1000],lw=1,label='x-T-S-R')\n",
        "plt.title('Residuo x - T - S - R')\n",
        "plt.legend()\n",
        "plt.xlabel('muestra')\n",
        "plt.savefig('x_T_S_D.png')\n",
        "plt.show()\n",
        "print('Error cuadrático medio:',np.std(x_3))"
      ],
      "metadata": {
        "id": "P6VG2KRvLhyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3jpljCUz3os"
      },
      "source": [
        "## Sábados, domingos y feriados\n",
        "\n",
        "Entre los distintos factores que afectan a la demanda, la actividad económica y el comportamiento de la población juegan un rol muy importante. Eso ya es obvio en los patrones diarios y semanales que observamos.\n",
        "Es de esperar entonces que la demanda sea distinta dependiendo si el día es laborable o no, total o parcialmente. Los sábados y domingos son libres para mucha gente. Los feriados, también. Veamos si efectivamente es así. Para comprobarlo, tomaremos ahora unos cuantos ejemplos de distintos tipos de día, a ver si las diferencias son notorias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQrujOHQz7dG"
      },
      "source": [
        "import numpy.random as rng\n",
        "\n",
        "def get_demanda(datos,num_dia):\n",
        "  return datos['AVGDEM'][num_dia*144:(num_dia+1)*144]\n",
        "\n",
        "def get_feriado(datos,num_dia):\n",
        "  return datos['HOLIDAY'][num_dia*144]\n",
        "\n",
        "def get_diasem(datos,num_dia):\n",
        "  return datos['ISODAY'][num_dia*144]\n",
        "\n",
        "\n",
        "def get_dias_al_azar(datos,ndias,dia_sem,feriado):\n",
        "  X = np.zeros((ndias,144))\n",
        "  n = len(datos)//144\n",
        "  for j in range(ndias):\n",
        "    ok = False\n",
        "    while not ok:\n",
        "      d = rng.randint(n)\n",
        "      dia = get_diasem(datos,d)\n",
        "      if feriado is not None and feriado != get_feriado(datos,d):\n",
        "        continue\n",
        "      if dia_sem is not None and get_diasem(datos,d) != dia_sem:\n",
        "        continue\n",
        "      X[j,:] = get_demanda(datos,d)\n",
        "      ok = True\n",
        "  return X\n",
        "\n",
        "rng.seed(123456)\n",
        "\n",
        "luneses  = get_dias_al_azar(dem_data,20,2,False)\n",
        "sabados  = get_dias_al_azar(dem_data,20,7,False)\n",
        "domingos = get_dias_al_azar(dem_data,20,1,False)\n",
        "feriados = get_dias_al_azar(dem_data,20,None,True)\n",
        "#\n",
        "# vamos a quitarle el valor medio para concentrarnos solo en el patron\n",
        "# de variación horario\n",
        "# nos conviene transponer los datos por cuestiones técnicas\n",
        "#\n",
        "luneses  = luneses.T  - np.mean(luneses,axis=1).T\n",
        "sabados  = sabados.T  - np.mean(sabados,axis=1).T\n",
        "domingos = domingos.T - np.mean(domingos,axis=1).T\n",
        "feriados = feriados.T - np.mean(feriados,axis=1).T\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(luneses,color='blue',alpha=0.2)\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('lunes')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(sabados,color='green',alpha=0.2)\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('sabado')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(domingos,color='orange',alpha=0.2)\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('domingos')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(feriados,color='magenta',alpha=0.2)\n",
        "plt.xlabel('muestra')\n",
        "plt.ylabel('demanda')\n",
        "plt.title('feriados')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ_-7m8p8T0x"
      },
      "source": [
        "## Comentarios sobre los días\n",
        "\n",
        "De  las gráficas anteriores podemos deducir:\n",
        "\n",
        "1.   los lunes son siempre iguales y aburridos\n",
        "1.   los sábados también son parecidos entre sí, y bastante distintos a los lunes\n",
        "1.   los domingos son distintos a los lunes y a los domingos, y en general se parecen bastante entre sí, pero hay algún que otro caso raro\n",
        "1.   los feriados también son distintos a los lunes, pero no son tan homogéneos: algunos se parecen a  sábados, otros a los domingos, y otros no se parecen a nada.\n",
        "\n",
        "Lo último no debería sorprender, ya que es claro que los feriados son distintos según el día de la semana en que caigan.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resolución del problema de predicción\n",
        "\n",
        "Nuestro objetivo es predecir la demanda de un día entero (24hs) en base a información disponible del día previo, que incluye principalmente la demanda de ese día, la temperatura, la radiación solar, y qué tipo de día es (dia de semana, feriado, etc.).\n",
        "\n",
        "Para lo anterior utilizaremos dos métodos.\n",
        "\n",
        "El primero, muy sencillo, utilizado como _baseline_, es conocido como el método de  _Hong_. Este método realiza una regresión no lineal entre la temperatura y el consumo para predecir la demanda.\n",
        "\n",
        "E segundo utiliza una red neuronal multicapa (no profunda) para realizar una regresión no lineal, multivariada, entre la demanda y temperatura del día anterior, y el día siguiente.\n",
        "\n",
        "<a id=\"evaluacion\"></a>\n",
        "## Evaluación\n",
        "\n",
        "Para evaluar el desempeño de nuestros modelos usaremos el _error proporcional absoluto medio_ (mean absolute proportional error, MAPE):\n",
        "\n",
        "$$ \\mathrm{MAPE} = \\frac{100}{n}\\sum_{j=1}^n\\left|\\frac{\\hat{y}_j-y_j}{y_j}\\right|$$\n",
        "\n"
      ],
      "metadata": {
        "id": "GCqELZNN8IBx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiQ2OfyUBkUz"
      },
      "source": [
        "\n",
        "\n",
        "<a id=\"preprocesamiento\"></a>\n",
        "## Preprocesamiento\n",
        "\n",
        "El mayor problema con los datos de este tipo son los _datos faltantes_. Por diversas razones, hay momentos en los que no puede tomarse una o varias medidas. Esto sucede con todas las medidas involucradas: demanda eléctrica, temperatura, radiación solar, viento, etc.\n",
        "\n",
        "Desafortunadamente, no siempre se dispone de información acerca de si una medida es válida o no: esto debe inferirse en base a los propios datos, y es una de las tareas más desafiantes en esta área de trabajo. En nuestro caso, esto ya está resuelto: el archivo que leimos, `demanda_2007_2012_pre1.csv`, **ya tiene identificados los datos que son potencialmente inválidos**.\n",
        "\n",
        "En general, la falta de datos se da en períodos de varias muestras consecutivas. Por ejemplo, si se rompe un medidor de temperatura, todas sus lecturas serán inválidas hasta que se arregle. Claramente, mientras más largo el período de la falla, peor será nuestra estimación de los datos que se perdieron. Si son muy pocos datos, por ejemplo dos, o tres, es posible obtener una muy buena estimación mediante una _interpolación_. Si más de, digamos, 5,  hay que recurrir a métodos más sofisticados.\n",
        "\n",
        "Abajo vemos el resultado de _interpolar_  los datos de temperatura del día 5 de febrero de 2007. Ese es el día 1 de la semana 6 de 2007 (isoday 1, isoweek 6), por lo que lo vemos a partir de la posición $5{\\times}7{\\times}144$.\n",
        "\n",
        "Los puntos grises son los datos que tenemos; ver que en varios lugares faltan datos. La curva naranja son los datos completados. Como se dijo, para estos casos sencillos, se obtienen muy buenos resultados utilizando métodos básicos de interpolación polinomial.\n",
        "\n",
        "**No mostraremos en este notebook cómo realizar dicha interpolación. Podemos proveer a quien esté interesado el código para realizar dicha interpolación.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6YD6hrMBxwW"
      },
      "source": [
        "demanda_cruda = dem_data\n",
        "#\n",
        "# leemos el archivo corregido\n",
        "#\n",
        "with gzip.open('demanda_2007_2012_pre4.csv.gz',mode='rt') as gzfile:\n",
        "  demanda = pd.read_csv(gzfile,delimiter='\\t',na_values=('-100.00','-200.00'))\n",
        "#\n",
        "# marcamos los datos inválidos con 0's, para que se vean\n",
        "#\n",
        "#demanda_cruda['TEMP'][demanda_cruda['TEMP'] < 0] = 0\n",
        "plt.figure(figsize=(12,6))\n",
        "offset = 144*7*5\n",
        "plt.plot(demanda_cruda['TEMP'][offset:(offset+144)],'o',label='conocido',lw=2,color='gray',alpha=0.5)\n",
        "plt.plot(demanda['TEMP'][offset:(offset+144)],label='interpolado',color='orange',lw=2,alpha=0.5)\n",
        "plt.ylim(15,30)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversión a frecuencia horaria\n",
        "\n",
        "Un pequeño paso que realizaremos es bajar la frecuencia de muestreo de diez minutos a una hora. En la práctica, no es necesario conocer la demanda con tanta precisión horaria.\n",
        "\n",
        "Esto puede parecer un simple paso de conveniencia pero es importante resaltar que muchas veces hay reticencia en descartar datos por miedo a obtener resultados peores. Es importante tener conocimiento del problema para saber cuánta precisión es relevante. Eso puede ahorrar muchos recursos y acelerar el proceso de desarrollo y experimentación dramáticamente!\n"
      ],
      "metadata": {
        "id": "QnFLU4Ab92xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convertir_a_horarios(datos,campo):\n",
        "  return np.mean(np.array(datos[campo]).reshape(-1,6),axis=1)\n"
      ],
      "metadata": {
        "id": "dsbmjVYN9zJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLrGcCqcBkZ-"
      },
      "source": [
        "\n",
        "## Método de Hong\n",
        "\n",
        "Los métodos existentes para predecir demanda son muchos. Varios de ellos utilizan lo anterior, de diversas formas. Uno simple y razonablemente efectivo que se utiliza como _referencia_ o _benchmark_ en la evaluación de nuevos métodos es el llamado _Hong_. El método Hong es una regresión lineal multivariada en donde sólo juegan los datos de calendario y la temperatura, combinados de diversas formas. La fórmula en cuestión es:\n",
        "\n",
        "$$ y_j = a_0 + a_1 j + a_2 d_j{\\times}h_j + a_3 m_j + a_4 m_j{\\times}t_j + a_5 m_j{\\times}t_j^2 + a_6 m_j{\\times}t_j^3 + a_7 h_j{\\times}t_j + a_8 h_j{\\times}t_j^2 + a_9 h_j{\\times}t_j^3\n",
        "$$\n",
        "\n",
        "en donde $j$ es el índice temporal, $m$ es el mes del año, $d$ es el día de la semana, $h$ es la hora del día, y $t$ es la temperatura.\n",
        "\n",
        "Lo primero que vamos a hacer entonces es construir este modelo de referencia mediante mínimos cuadrados en base a los datos que tenemos. Para eso calculamos un vector de características $\\mathbf{x}_j$ de la siguiente manera:\n",
        "\n",
        "$$\\mathbf{x}_j =  (1,j,d_jh_j,m_j,m_jt,m_jt_j^2,m_jt_j^3,h_jt_j,h_jt_j^2,h_jt_j^3)$$\n",
        "\n",
        "tomamos la demanda actual como $y_j$, y resolvemos el problema de regresión:\n",
        "\n",
        "$$\\mathbf{a}^* = \\arg\\min_{\\mathbf{a}} \\|\\mathbf{Xa} - \\mathbf{y}\\|.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcnDnuyLJUjA"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
        "\n",
        "def convertir_a_horarios(datos,campo):\n",
        "  return np.mean(np.array(datos[campo]).reshape(-1,6),axis=1)\n",
        "\n",
        "def hong_features(datos):\n",
        "  #\n",
        "  # reducimos a frecuencia horaria, como hicimos con las redes\n",
        "  #\n",
        "  n = len(datos)//6\n",
        "  c = np.ones(n)\n",
        "  j = np.arange(n)\n",
        "  m = np.array(datos['MON']-1)[::6]  # de 0 a 11\n",
        "  h = np.array(datos['HOUR'])[::6]\n",
        "  d = np.array(datos['ISODAY']-1)[::6] # de 0 a 6\n",
        "  t = convertir_a_horarios(datos,'TEMP')\n",
        "  y = convertir_a_horarios(datos,'AVGDEM')\n",
        "  X = np.zeros((n,10))\n",
        "  X[:,0] = c\n",
        "  X[:,1] = j\n",
        "  X[:,2] = d*h\n",
        "  X[:,3] = m\n",
        "  X[:,4] = m*t\n",
        "  X[:,5] = m*t**2\n",
        "  X[:,6] = m*t**3\n",
        "  X[:,7] = h*t\n",
        "  X[:,8] = h*t**2\n",
        "  X[:,9] = h*t**3\n",
        "  return X,y\n",
        "\n",
        "\n",
        "\n",
        "X,y = hong_features(demanda)\n",
        "#\n",
        "# reescalamos los datos para tener mejor condicionamiento\n",
        "#\n",
        "X = X / np.sqrt(np.mean(X**2,axis=0))\n",
        "\n",
        "ntest = 365*24\n",
        "X_train_hong = X[:-ntest,:]\n",
        "y_train_hong = y[:-ntest]\n",
        "\n",
        "X_test_hong = X[-ntest:,:]\n",
        "y_test_hong = y[-ntest:]\n",
        "\n",
        "hong = Lasso(alpha=1e-2,max_iter=10000,tol=1e-4)\n",
        "res = hong.fit(X_train_hong,y_train_hong)\n",
        "ypred_hong = hong.predict(X_test_hong)\n",
        "yerr_hong  = ypred_hong - y_test_hong\n",
        "\n",
        "print(\"\\n==============================\\n\")\n",
        "\n",
        "print(\"RESULTADOS PRIMARIOS:\\n\")\n",
        "print(\"\\tMODELO HONG:\")\n",
        "print('\\t\\tMAPE:',np.round(100*np.mean(np.abs(yerr_hong/y_test_hong)),2),'%')\n",
        "\n",
        "\n",
        "print(\"\\n==============================\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcT7nhRIBkXZ"
      },
      "source": [
        "<a id=\"modelado\"></a>\n",
        "# Modelo neuronal\n",
        "\n",
        "Nuestro objetivo es el de proveer una buena predicción para la demanda de las próximas 24 horas en base a tres factores:\n",
        "\n",
        "1.   los datos que tenemos hasta el día anterior\n",
        "1.   la información de calendario disponible, incluyendo fecha, día de la semana, y si es feriado\n",
        "1.   el pronóstico de temperatura para las próximas 24 horas\n",
        "\n",
        "Por otro lado, la información a priori que tenemos es la siguiente:\n",
        "\n",
        "1. peridicidad diaria\n",
        "1. periodicidad semanal\n",
        "1. periodicidad anual\n",
        "1. tendencia creciente a lo largo del tiempo\n",
        "1. los días feriados el comportamiento es distinto al de los días normales\n",
        "1. la temperatura influye en el consumo (a través de calefacción y/o refrigeración); eso sí: no es claro cómo\n",
        "\n",
        "En el paper mencionado al principio se describen varios métodos para atacar el problema. El método que vamos a ensayar aquí es distinto y hace uso de redes neuronales  cuya entrada son todos los datos anteriores. Para esto usaremos las funciones de `sklearn`  para entrenar redes neuronales \"comunes\".\n",
        "\n",
        "Más precisamente, nos interesa estimar la salida de un día _entero_ dada toda la información disponible. Eso nos da una salida de dimensión $144$.\n",
        "\n",
        "Claramente, no podemos entrenar una red neuronal que tome _todo_ el pasado en cuenta para la entrada. Lo que haremos es incorporar como entradas los datos de demanda y temperatura del día anterior al que queremos predecir. Esto son dos vectores adicionales de $144$ muestras cada uno.\n",
        "\n",
        "El problema que tenemos ahora es que eso puede darnos una red demasiado grande, con demasiado detalle: no necesitamos la demanda estimada cada 10 minutos: alcanza con 1 hora. Podemos entonces simplificarnos mucho la vida si reducimos nuestros datos a una escala _horaria_. Así, pasamos a tener $24$ salidas para estimar, y poco más de $48$ entradas (24 temperaturas, 24 demandas, y datos de calendario).\n",
        "\n",
        "Finalmente, nuestros datos están dados a una muestra diezminutal por fila. Necesitamos colapsar de a 144 filas en una sola muestra. Esto no es parte del modelado en sí, pero es necesario programar un poco para hacer esta transformación.\n",
        "\n",
        "\n",
        "**EJERCICIO: pruebe modificar los parámetros de la red neuronal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SACY4GiNjO-y"
      },
      "source": [
        "def transform_data(datos):\n",
        "  #\n",
        "  #\n",
        "  #\n",
        "  nmuestras = len(datos)\n",
        "  ndias     = nmuestras // 144\n",
        "  nhoras    = 24\n",
        "  #\n",
        "  # promediamos 6 datos diezminutales en 1 de una hora\n",
        "  #\n",
        "  demanda   = datos['AVGDEM'] # 144 por dia\n",
        "  Xdem      = np.array(demanda).reshape(ndias,nhoras,-1)\n",
        "  Xdem      = np.mean(Xdem,axis=2)\n",
        "  #\n",
        "  # idem con la temperatura\n",
        "  #\n",
        "  temp      = datos['TEMP']   # 144 por dia\n",
        "  Xtem      = np.array(temp).reshape(ndias,nhoras,-1)\n",
        "  Xtem      = np.mean(Xtem,axis=2)\n",
        "  #\n",
        "  # nos quedamos con 1 dato de calendario por día\n",
        "  #\n",
        "  #\n",
        "  # la hora no la usamos; ya está implícita en el índice de la demanda\n",
        "  #\n",
        "  Xday = np.array(datos['ISODAY']).reshape(ndias,-1)\n",
        "  Xday = Xday[:,1].reshape(-1,1)\n",
        "  Xsem = np.array(datos['ISOWEEK']).reshape(ndias,-1)\n",
        "  Xsem = Xsem[:,1].reshape(-1,1)\n",
        "  Xhol = np.array(datos['HOLIDAY']).reshape(ndias,-1)\n",
        "  Xhol = Xhol[:,1].reshape(-1,1)\n",
        "\n",
        "  #\n",
        "  # armamos X con demandas, temperatura, y todo lo demás\n",
        "  #\n",
        "  X = np.concatenate((Xdem[:-1,:],Xtem[:-1,:],Xday[1:,:],Xsem[1:,:],Xhol[1:,:]),axis=1)\n",
        "  #\n",
        "  # los X que podemos usar son los de todos los días menos el último (para el cual\n",
        "  # no tenemos el dato del día siguiente)\n",
        "  #\n",
        "\n",
        "  #\n",
        "  # el vector de respuestas que buscamos es el de las 24 medidas del día siguiente\n",
        "  #\n",
        "  Y = Xdem[1:,:]\n",
        "  return X, Y\n",
        "\n",
        "X,Y = transform_data(demanda)\n",
        "\n",
        "#\n",
        "# evaluamos en el ultimo año\n",
        "#\n",
        "ndias_test = 365\n",
        "ntest = ndias_test\n",
        "n = X.shape[0]\n",
        "ndias = n\n",
        "\n",
        "X_train = X[:(n-ntest),:]\n",
        "Y_train = Y[:(n-ntest),:]\n",
        "X_test  = X[(n-ntest):,:]\n",
        "Y_test  = Y[(n-ntest):,:]\n",
        "\n",
        "# red neuronal para clasificación. MLP = Multi Layer Perceptron\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "#\n",
        "# EJERCICIO: experimentar con distintos parámetros!\n",
        "#\n",
        "nnet = MLPRegressor(solver='adam', max_iter=1000,\n",
        "                    random_state=1234,\n",
        "                    learning_rate_init=1e-3,\n",
        "                    hidden_layer_sizes=[250,50],\n",
        "                    activation='relu',\n",
        "                    batch_size=250,\n",
        "                    early_stopping=True,tol=1e-5)\n",
        "#\n",
        "# ajustamos el modelo a los datos de entrenamiento\n",
        "#\n",
        "nnet.fit(X_train, Y_train)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.semilogy(nnet.validation_scores_)\n",
        "plt.grid(True)\n",
        "plt.xlabel('iteraciones')\n",
        "plt.ylabel('score')\n",
        "plt.title('evolución del aprendizaje')\n",
        "plt.ylim(0.1,1.0)\n",
        "print(nnet.best_validation_score_)\n",
        "\n",
        "Ypred_nnet = nnet.predict(X_test)\n",
        "Yerr_nnet  = Ypred_nnet - Y_test\n",
        "ypred_nnet  = Ypred_nnet.ravel()\n",
        "yerr_nnet  = Yerr_nnet.ravel()\n",
        "\n",
        "print(\"\\n\\tMODELO NEURONAL:\")\n",
        "print('\\t\\tMAPE:',np.round(100*np.mean(np.abs(yerr_nnet/y_test_hong)),2),'%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8RXqfYzBp4u"
      },
      "source": [
        "## Inspección de los resultados\n",
        "\n",
        "Bueno! Nuestro modelo neuronal, sin muchas vueltas, dio en principio muchísimo mejor que el de Hong.\n",
        "\n",
        "Es cierto que el modelo de Hong es muy sencillo, y no toma en cuenta cosas como los feriados. Sin embargo, el modelo de Hong tiene una gran ventaja: utiliza la información de temperatura del mismo día que se desea estimar la demanda. En ese sentido, Hong ni siquiera es un modelo predictivo. Lo sería si se basara en pronósticos de temperatura, pero ese no es nuestro caso.\n",
        "\n",
        "En contraste, el modelo neuronal sólo utiliza información del día previo. Es un modelo realmente predictivo.\n",
        "\n",
        "Ahora, lo que realmente interesa es ver el desempeño diario. Las medidas que tomamos anteriormente eran globales. Vamos a calcular ahora el MAPE para cada dia del último año, y graficarlo para cada método.\n",
        "\n",
        "Luego vamos a elegir 5 casos representativo de cada método: el peor día (percentil 100 de error), el percentil 75, el 50 (mediana), el 25 y el mejor día (percentil 0 de error).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmOKad0aDuze"
      },
      "source": [
        "Ypred_hong = np.reshape(ypred_hong,(-1,24))\n",
        "Yerr_hong = yerr_hong.reshape(-1,24)\n",
        "mape_net  = 100*np.mean(np.abs(Yerr_nnet/Y_test),axis=1)\n",
        "mape_hong = 100*np.mean(np.abs(Yerr_hong/Y_test),axis=1)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(mape_net,label='nnet')\n",
        "plt.plot(mape_hong,label='Hong')\n",
        "plt.grid(True)\n",
        "plt.xlabel('dia')\n",
        "plt.ylabel('MAPE diario')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr9hg_n-GJbo"
      },
      "source": [
        "## Mejores y peores resultados\n",
        "\n",
        "Veamos ahora los mejores y los peores días en detalle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbzdQPklGQfc"
      },
      "source": [
        "net_idx  = np.argsort(mape_net)\n",
        "hong_idx = np.argsort(mape_hong)\n",
        "n = len(net_idx)\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "j = 1\n",
        "for p in (0,25,50,75,100):\n",
        "  rank = p*(n-1)//100\n",
        "  idx = net_idx[rank]\n",
        "  mape = mape_net[idx]\n",
        "\n",
        "  net = Ypred_nnet[idx,:]\n",
        "  ref = Y_test[idx,:]\n",
        "  plt.subplot(2,5,j)\n",
        "  plt.plot(net,label='net')\n",
        "  plt.plot(ref,label='real')\n",
        "  plt.grid(True)\n",
        "  plt.xlabel('hora')\n",
        "  plt.ylabel('demanda')\n",
        "  plt.title(f'percentil {p} mape {mape:4.1f}')\n",
        "  plt.legend()\n",
        "  j = j + 1\n",
        "\n",
        "for p in (0,25,50,75,100):\n",
        "  rank = p*(n-1)//100\n",
        "  mape=mape_hong[hong_idx[rank]]\n",
        "  hong = Ypred_hong[hong_idx[rank],:]\n",
        "  ref = Y_test[hong_idx[rank],:]\n",
        "  plt.subplot(2,5,j)\n",
        "  plt.plot(hong,label='hong')\n",
        "  plt.plot(ref,label='real')\n",
        "  plt.grid(True)\n",
        "  plt.xlabel('hora')\n",
        "  plt.ylabel('demanda')\n",
        "  plt.title(f'percentil {p}: mape={mape:4.1f}')\n",
        "  plt.legend()\n",
        "  j = j + 1\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IW6dxY2Cs48"
      },
      "source": [
        "### Desempeño según tipo de día\n",
        "\n",
        "Vamos ahora a distinguir el desempeño según los tipos de día que observamos:\n",
        "\n",
        "*   dia de semana\n",
        "*   fin de semana\n",
        "*   feriado\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Df1XHE5C76w"
      },
      "source": [
        "diasem_test = X_test[:,-3]\n",
        "hol_test    = X_test[:,-1].astype(bool)\n",
        "finde_test  = np.logical_or(np.equal(diasem_test,0),np.equal(diasem_test,6))\n",
        "finde_test  = np.logical_and(finde_test,np.logical_not(hol_test))\n",
        "comun_test  = np.logical_and(np.logical_not(finde_test),np.logical_not(hol_test))\n",
        "\n",
        "#Y_test_finde = Y_test[finde_test,:]\n",
        "#Y_test_comun = Y_test[comun_test,:]\n",
        "#Y_test_hol   = Y_test[hol_test,:]\n",
        "\n",
        "mape_finde = mape_net[finde_test]\n",
        "mape_comun = mape_net[comun_test]\n",
        "mape_hol   = mape_net[hol_test]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "bins = np.arange(-0.5,20,1)\n",
        "plt.hist(mape_comun,bins,alpha=0.3,color='cyan',label='entresemana',density=True)\n",
        "plt.hist(mape_finde,bins,alpha=0.3,color='yellow',label='finde',density=True)\n",
        "plt.hist(mape_hol,bins,alpha=0.3,color='magenta',label='feriado',density=True)\n",
        "plt.legend()\n",
        "plt.xlabel('MAPE')\n",
        "plt.ylabel('probabilidad empírica')\n",
        "plt.title('histograma de MAPE según tipo de día')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xchZUUqBHOm"
      },
      "source": [
        "Los tres histogramas nos muestran algo que ya podíamos prever en la visualización inicial: los días feriados son más difíciles de modelar que los otros, y los días de semana son los más predecibles. Claro que hay excepciones: hay un día feriado que es predicho muy bien, y hay al menos un día de semana  que tuvo una muy mala predicción. Cabe recordar que en 2012 fueron los juegos olímpicos de Londres. Es posible que algún evento importante relacionado a los Juegos Olímpicos haya ocurrido entresemana, distorsionando así el comportamiento habitual de la población.\n",
        "\n",
        "## Comentarios finales\n",
        "\n",
        "Los resultados obtenidos son extremadamente buenos, más teniendo en cuenta que se obtuvieron de manera casi automática, con una red neuronal, sin aplicar mucho conocimiento a priori. Si bien el modelo _mezcla_ que se describe en [[CCMR2018]](#CCMR2018) da mejores resultados en casi todos los casos que el neuronal, ninguno de los modelos individuales descritos allí, varios de ellos bastante sofisticados, tienen un desempeño tan bueno como el que obtuvimos aquí con una red neuronal, casi sin ajustar sus hiperparámetros parámetros (de todos modos, cabe notar que los resultados no son sobre el mismo año).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewCdoeDY_Qbx"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "<a id=\"CCMR2018\">CCMR2018</a>\n",
        "Castrillejo A., Cugliari J., Massa F., Ramirez I. (2018) Electricity Demand Forecasting: The Uruguayan Case. Renewable Energy: Forecasting and Risk Management. FRM 2017. Springer Proceedings in Mathematics & Statistics, vol 254. Springer, Cham. https://doi.org/10.1007/978-3-319-99052-1_6.\n",
        " [PREPRINT](https://hal.archives-ouvertes.fr/hal-01787143/document).\n"
      ]
    }
  ]
}